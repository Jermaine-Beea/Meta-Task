{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# META-Powered PDF Question & Answer Assistant\n",
    "*Build a simple RAG system using Meta Llama models*\n",
    "\n",
    "This notebook will help you build an AI assistant that can read any PDF you give it and answer questions **only** based on that PDF.\n",
    "\n",
    "You will:\n",
    "- Load a PDF (e.g. report, policy, curriculum, research paper)\n",
    "- Ask natural questions about it\n",
    "- Get short, accurate answers grounded in the document\n",
    "\n",
    "The AI uses **Retrieval-Augmented Generation (RAG)**: it searches the PDF first, then answers using only what it finds (no guessing).\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "Before you start, make sure you have:\n",
    "1. **Google Colab access** (you're already here!)\n",
    "2. **OpenRouter API key** - Follow this video to get one: https://www.youtube.com/watch?v=-X9DVzzxpAA\n",
    "3. **A Google Drive link to your PDF** (set to \"Anyone with the link\" or accessible to your account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Install Required Libraries\n",
    "\n",
    "In this step, you install all the tools your AI assistant needs.\n",
    "Run this cell **once per Colab session**.\n",
    "\n",
    "**What you install:**\n",
    "- `llama-index` â€“ framework for reading, chunking, indexing and querying documents\n",
    "- `llama-index-llms-openrouter` â€“ connects to Meta Llama models via OpenRouter\n",
    "- `llama-index-embeddings-huggingface` â€“ creates embeddings for semantic search\n",
    "- `llama-index-readers-file` â€“ reads PDFs and other files\n",
    "- `llama-index-packs-fusion-retriever` â€“ Meta \"Query Fusion\" retriever pack\n",
    "- `sentence-transformers` â€“ semantic understanding and chunking\n",
    "- `nest-asyncio` â€“ fixes async issues in Colab\n",
    "- `requests` â€“ downloads the PDF from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… Installation complete\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install all required libraries\n",
    "%pip install -q \\\n",
    "  llama-index \\\n",
    "  llama-index-llms-openrouter \\\n",
    "  llama-index-embeddings-huggingface \\\n",
    "  llama-index-readers-file \\\n",
    "  llama-index-packs-fusion-retriever \\\n",
    "  sentence-transformers \\\n",
    "  nest-asyncio \\\n",
    "  requests\n",
    "\n",
    "print(\"âœ… Installation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Connect to the AI Model\n",
    "\n",
    "Here you:\n",
    "- Import core libraries\n",
    "- Enter your **OpenRouter API key**\n",
    "- Configure the **Llama model**\n",
    "- Configure the **embedding model**\n",
    "- Tell `llama-index` to use them\n",
    "\n",
    "Run this cell **after** Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AI model and settings are ready to use\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Connect to the AI model\n",
    "import os\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Ask for your OpenRouter API key (input is hidden like a password)\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OpenRouter API key: \")\n",
    "\n",
    "# Configure the LLM (Meta Llama via OpenRouter)\n",
    "llm = OpenRouter(\n",
    "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "    model=\"meta-llama/llama-3.3-70b-instruct:free\",\n",
    "    max_tokens=512,\n",
    "    temperature=0.1,  # Low = more precise, less \"creative\"\n",
    "    timeout=60,\n",
    "    system_prompt=(\n",
    "        \"You are an expert RAG system that answers ONLY using the provided context. \"\n",
    "        \"Never hallucinate. Never guess. If the answer is not in the context, say so. \"\n",
    "        \"Provide short, clear, factual responses with 2â€“4 evidence bullets.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Configure the embedding model\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "# Register both with LlamaIndex settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "print(\"âœ… AI model and settings are ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Download the PDF from Google Drive\n",
    "\n",
    "This step:\n",
    "1. Asks you for a **Google Drive link** to your PDF\n",
    "2. Extracts the **file ID** from the link\n",
    "3. Downloads the PDF into a local `data/` folder\n",
    "4. Saves it as `data/source.pdf`\n",
    "\n",
    "Supported link formats include:\n",
    "- `https://drive.google.com/file/d/<FILE_ID>/view?...`\n",
    "- `https://drive.google.com/open?id=<FILE_ID>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Downloading PDF (file ID 1aIXmnTlGSvlhpjn29m_27mCNcawlamkQ)...\n",
      "âœ… PDF downloaded â†’ data/source.pdf\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Download the PDF from Google Drive\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def download_pdf_from_drive(drive_url: str, save_path: str):\n",
    "    \"\"\"\n",
    "    Download a PDF from a Google Drive sharing link and save it locally.\n",
    "    \"\"\"\n",
    "    # Try pattern: /d/<FILE_ID>/\n",
    "    match = re.search(r\"/d/([A-Za-z0-9_-]+)\", drive_url)\n",
    "    if match:\n",
    "        file_id = match.group(1)\n",
    "    else:\n",
    "        # Try pattern: ?id=<FILE_ID>\n",
    "        match = re.search(r\"id=([A-Za-z0-9_-]+)\", drive_url)\n",
    "        if match:\n",
    "            file_id = match.group(1)\n",
    "        else:\n",
    "            raise ValueError(\"âŒ Could not extract file ID from the link.\")\n",
    "\n",
    "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "    print(f\"ğŸ“¥ Downloading PDF (file ID {file_id})...\")\n",
    "\n",
    "    resp = requests.get(download_url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "    print(f\"âœ… PDF downloaded â†’ {save_path}\")\n",
    "\n",
    "# Ask for the Drive link\n",
    "drive_link = input(\"ğŸ“Œ Paste your Google Drive PDF link here: \").strip()\n",
    "\n",
    "# Make sure the data folder exists\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Local path for the PDF\n",
    "pdf_path = os.path.join(DATA_DIR, \"source.pdf\")\n",
    "\n",
    "# Download the PDF\n",
    "download_pdf_from_drive(drive_link, pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Break the PDF into Semantic Chunks\n",
    "\n",
    "The AI cannot use one giant block of text.\n",
    "Here you:\n",
    "- Load the PDF\n",
    "- Use a **semantic splitter** to create \"smart\" chunks (not random splits)\n",
    "- Label each chunk with simple metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:50:11,148 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loaded 1 document(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:50:14,048 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Created 2 high-quality semantic nodes.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Break the PDF into semantic chunks\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Load the PDF as a document\n",
    "documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
    "print(f\"ğŸ“„ Loaded {len(documents)} document(s).\")\n",
    "\n",
    "# Embedding model for semantic splitting (can reuse the same model name)\n",
    "semantic_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Create a semantic splitter\n",
    "parser = SemanticSplitterNodeParser(\n",
    "    buffer_size=3,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=semantic_embed,\n",
    ")\n",
    "\n",
    "# Generate semantic nodes (chunks)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# Add simple metadata to each chunk\n",
    "for n in nodes:\n",
    "    n.metadata[\"source\"] = pdf_path\n",
    "    n.metadata[\"chunk_type\"] = \"semantic\"\n",
    "\n",
    "print(f\"ğŸ” Created {len(nodes)} high-quality semantic nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Build the Query Fusion Retriever\n",
    "\n",
    "Now you build the **search engine** that powers your RAG system.\n",
    "It uses **Query Fusion**:\n",
    "- Rewrites your question several ways\n",
    "- Searches multiple times\n",
    "- Fuses the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /workspaces/Meta-Task/query_rewriting_pack\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-packs-fusion-retriever==0.5.1) (0.14.10)\n",
      "Requirement already satisfied: llama-index-retrievers-bm25<0.7,>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-packs-fusion-retriever==0.5.1) (0.6.5)\n",
      "Requirement already satisfied: rank-bm25<0.3,>=0.2.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-packs-fusion-retriever==0.5.1) (0.2.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.13.2)\n",
      "Requirement already satisfied: aiosqlite in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2025.12.0)\n",
      "Requirement already satisfied: httpx in /home/codespace/.local/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.11.5)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/codespace/.local/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.6.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.9.2)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.3.5)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (12.0.0)\n",
      "Requirement already satisfied: platformdirs in /home/codespace/.local/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.12.5)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/codespace/.local/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /home/codespace/.local/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.0.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.22.0)\n",
      "Requirement already satisfied: griffe in /usr/local/python/3.12.1/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.1.6)\n",
      "Requirement already satisfied: bm25s>=0.2.7.post1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-retrievers-bm25<0.7,>=0.6.0->llama-index-packs-fusion-retriever==0.5.1) (0.2.14)\n",
      "Requirement already satisfied: pystemmer<3,>=2.2.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-retrievers-bm25<0.7,>=0.6.0->llama-index-packs-fusion-retriever==0.5.1) (2.2.0.3)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.4.2)\n",
      "Requirement already satisfied: idna>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.11)\n",
      "Requirement already satisfied: scipy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from bm25s>=0.2.7.post1->llama-index-retrievers-bm25<0.7,>=0.6.0->llama-index-packs-fusion-retriever==0.5.1) (1.16.3)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2025.11.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (2025.11.12)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/codespace/.local/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (25.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /home/codespace/.local/lib/python3.12/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.4.6)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-packs-fusion-retriever==0.5.1) (3.0.3)\n",
      "Building wheels for collected packages: llama-index-packs-fusion-retriever\n",
      "  Building wheel for llama-index-packs-fusion-retriever (pyproject.toml): started\n",
      "  Building wheel for llama-index-packs-fusion-retriever (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-index-packs-fusion-retriever: filename=llama_index_packs_fusion_retriever-0.5.1-py3-none-any.whl size=4212 sha256=a2d8496a69253257c66e96963a254a9f4c20682b38c20d7d1616ebe5468b108a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n_adswfz/wheels/b6/4e/20/10de5911db2177fd003776d4fb23117dccd34fe2477c1f2d2b\n",
      "Successfully built llama-index-packs-fusion-retriever\n",
      "Installing collected packages: llama-index-packs-fusion-retriever\n",
      "  Attempting uninstall: llama-index-packs-fusion-retriever\n",
      "    Found existing installation: llama-index-packs-fusion-retriever 0.5.1\n",
      "    Uninstalling llama-index-packs-fusion-retriever-0.5.1:\n",
      "      Successfully uninstalled llama-index-packs-fusion-retriever-0.5.1\n",
      "Successfully installed llama-index-packs-fusion-retriever-0.5.1\n",
      "ğŸš€ Advanced Query Fusion RAG Engine Ready!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Build the Query Fusion retriever\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "# Download or load the Query Fusion pack\n",
    "QueryRewritingRetrieverPack = download_llama_pack(\n",
    "    \"QueryRewritingRetrieverPack\",\n",
    "    \"./query_rewriting_pack\",\n",
    ")\n",
    "\n",
    "# Create the advanced retriever using your nodes\n",
    "query_rewriting_pack = QueryRewritingRetrieverPack(\n",
    "    nodes,                      # semantic chunks from Step 4\n",
    "    chunk_size=256,\n",
    "    vector_similarity_top_k=8,\n",
    "    fusion_similarity_top_k=8,\n",
    "    num_queries=6,              # number of query rewrites\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ Advanced Query Fusion RAG Engine Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Ask Questions in an Interactive Loop\n",
    "\n",
    "Finally, you create a simple chat loop:\n",
    "- Type a question about the PDF\n",
    "- The system runs the RAG pipeline\n",
    "- You see a clear answer\n",
    "- Type `end` to exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG Interactive Mode\n",
      "Ask any question about your PDF.\n",
      "Type 'end' to exit.\n",
      "\n",
      "\n",
      "ğŸ” Retrieving answer...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:50:39,246 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "What is the origin of coffee\n",
      "How is coffee made from bean to cup\n",
      "What are the different types of coffee drinks\n",
      "What are the health benefits and risks of drinking coffee\n",
      "How does coffee affect the brain and body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:50:43,589 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â“ QUESTION:\n",
      "what is coffee\n",
      "\n",
      "ğŸ§  ANSWER:\n",
      "Coffee is a brewed drink prepared from roasted coffee beans. \n",
      "* These beans are the seeds of berries from the Coffea plant.\n",
      "* They are valued for their stimulating caffeine content.\n",
      "* The drink can be prepared using various methods, such as Espresso or Pour-Over.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "ğŸ” Retrieving answer...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:50:56,296 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "What is coffee and its benefits\n",
      "History and origin of coffee beans\n",
      "Types of coffee drinks and recipes\n",
      "Coffee health effects and side effects\n",
      "How to brew coffee at home with different methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:50:59,150 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â“ QUESTION:\n",
      "what is coffe\n",
      "\n",
      "ğŸ§  ANSWER:\n",
      "Coffee is a brewed drink prepared from roasted coffee beans. \n",
      "* These beans are the seeds of berries from the Coffea plant.\n",
      "* They are valued for their stimulating caffeine content.\n",
      "* The drink can be prepared using various methods, such as Espresso or Pour-Over.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "ğŸ” Retrieving answer...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:51:17,640 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "exit strategies for business\n",
      "what does exit mean in programming\n",
      "emergency exit procedures\n",
      "exit interview questions and answers\n",
      "how to exit full screen mode on windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:51:20,235 - INFO - HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â“ QUESTION:\n",
      "exit\n",
      "\n",
      "ğŸ§  ANSWER:\n",
      "There is no information about \"exit\" in the context.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "ğŸ‘‹ Session ended.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Ask questions in an interactive loop\n",
    "def safe_rag_run(question, retries=3):\n",
    "    \"\"\"\n",
    "    Run the RAG pipeline with basic retry logic.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = query_rewriting_pack.run(question)\n",
    "\n",
    "            if resp is None or str(resp).strip() == \"\":\n",
    "                raise ValueError(\"Empty LLM response.\")\n",
    "\n",
    "            return resp\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error: {e}\")\n",
    "            print(f\"ğŸ” Retrying ({attempt+1}/{retries})...\")\n",
    "\n",
    "    return \"âŒ Could not generate a valid answer after retries.\"\n",
    "\n",
    "print(\"\\nRAG Interactive Mode\")\n",
    "print(\"Ask any question about your PDF.\")\n",
    "print(\"Type 'end' to exit.\\n\")\n",
    "\n",
    "# Interactive Q&A loop\n",
    "while True:\n",
    "    user_question = input(\"ğŸŸ¦ Enter your question: \").strip()\n",
    "\n",
    "    if user_question.lower() == \"end\":\n",
    "        print(\"\\nğŸ‘‹ Session ended.\")\n",
    "        break\n",
    "\n",
    "    print(\"\\nğŸ” Retrieving answer...\\n\")\n",
    "\n",
    "    # Run the question through the RAG pipeline\n",
    "    response = safe_rag_run(user_question)\n",
    "\n",
    "    print(\"\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(\"â“ QUESTION:\")\n",
    "    print(user_question)\n",
    "    print(\"\\nğŸ§  ANSWER:\")\n",
    "    print(response)\n",
    "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example Questions You Can Try\n",
    "\n",
    "Once everything is running, try questions like:\n",
    "\n",
    "- \"What are the main goals in this document?\"\n",
    "- \"What does this policy say about attendance?\"\n",
    "- \"Summarise the key points in chapter one.\"\n",
    "- \"List all the responsibilities of students mentioned in this document.\"\n",
    "- \"How is assessment described in this curriculum?\"\n",
    "\n",
    "For the Coffee Guide example:\n",
    "- \"What are the different coffee bean types?\"\n",
    "- \"How is cold brew made?\"\n",
    "- \"What are the health benefits of coffee?\"\n",
    "- \"Which country drinks the most coffee?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've successfully built a RAG-powered PDF Q&A assistant using Meta Llama models!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Save your work**: File â†’ Save a copy in Drive\n",
    "2. **Download the notebook**: File â†’ Download â†’ Download .ipynb\n",
    "3. **Push to GitLab**: Upload your completed notebook to your forked repository\n",
    "\n",
    "### Tips for GitLab:\n",
    "```bash\n",
    "# Add your notebook\n",
    "git add meta_task_pdf_qa.ipynb\n",
    "\n",
    "# Commit with a descriptive message\n",
    "git commit -m \"Complete META-powered PDF Q&A RAG system\"\n",
    "\n",
    "# Push to your fork\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Your Name  \n",
    "**Date:** December 12, 2025  \n",
    "**Framework:** LlamaIndex + Meta Llama 3.3 70B via OpenRouter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

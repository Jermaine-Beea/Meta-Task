{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# META-Powered PDF Question & Answer Assistant\n",
    "*Build a simple RAG system using Meta Llama models*\n",
    "\n",
    "This notebook will help you build an AI assistant that can read any PDF you give it and answer questions **only** based on that PDF.\n",
    "\n",
    "You will:\n",
    "- Load a PDF (e.g. report, policy, curriculum, research paper)\n",
    "- Ask natural questions about it\n",
    "- Get short, accurate answers grounded in the document\n",
    "\n",
    "The AI uses **Retrieval-Augmented Generation (RAG)**: it searches the PDF first, then answers using only what it finds (no guessing).\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "Before you start, make sure you have:\n",
    "1. **Google Colab access** (you're already here!)\n",
    "2. **OpenRouter API key** - Follow this video to get one: https://www.youtube.com/watch?v=-X9DVzzxpAA\n",
    "3. **A Google Drive link to your PDF** (set to \"Anyone with the link\" or accessible to your account)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Install Required Libraries\n",
    "\n",
    "In this step, you install all the tools your AI assistant needs.\n",
    "Run this cell **once per Colab session**.\n",
    "\n",
    "**What you install:**\n",
    "- `llama-index` ‚Äì framework for reading, chunking, indexing and querying documents\n",
    "- `llama-index-llms-openrouter` ‚Äì connects to Meta Llama models via OpenRouter\n",
    "- `llama-index-embeddings-huggingface` ‚Äì creates embeddings for semantic search\n",
    "- `llama-index-readers-file` ‚Äì reads PDFs and other files\n",
    "- `llama-index-packs-fusion-retriever` ‚Äì Meta \"Query Fusion\" retriever pack\n",
    "- `sentence-transformers` ‚Äì semantic understanding and chunking\n",
    "- `nest-asyncio` ‚Äì fixes async issues in Colab\n",
    "- `requests` ‚Äì downloads the PDF from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install all required libraries\n",
    "%pip install -q \\\n",
    "  llama-index \\\n",
    "  llama-index-llms-openrouter \\\n",
    "  llama-index-embeddings-huggingface \\\n",
    "  llama-index-readers-file \\\n",
    "  llama-index-packs-fusion-retriever \\\n",
    "  sentence-transformers \\\n",
    "  nest-asyncio \\\n",
    "  requests\n",
    "\n",
    "print(\"‚úÖ Installation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Connect to the AI Model\n",
    "\n",
    "Here you:\n",
    "- Import core libraries\n",
    "- Enter your **OpenRouter API key**\n",
    "- Configure the **Llama model**\n",
    "- Configure the **embedding model**\n",
    "- Tell `llama-index` to use them\n",
    "\n",
    "Run this cell **after** Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Connect to the AI model\n",
    "import os\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Ask for your OpenRouter API key (input is hidden like a password)\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OpenRouter API key: \")\n",
    "\n",
    "# Configure the LLM (Meta Llama via OpenRouter)\n",
    "llm = OpenRouter(\n",
    "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "    model=\"meta-llama/llama-3.3-70b-instruct:free\",\n",
    "    max_tokens=512,\n",
    "    temperature=0.1,  # Low = more precise, less \"creative\"\n",
    "    timeout=60,\n",
    "    system_prompt=(\n",
    "        \"You are an expert RAG system that answers ONLY using the provided context. \"\n",
    "        \"Never hallucinate. Never guess. If the answer is not in the context, say so. \"\n",
    "        \"Provide short, clear, factual responses with 2‚Äì4 evidence bullets.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Configure the embedding model\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "# Register both with LlamaIndex settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "print(\"‚úÖ AI model and settings are ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Download the PDF from Google Drive\n",
    "\n",
    "This step:\n",
    "1. Asks you for a **Google Drive link** to your PDF\n",
    "2. Extracts the **file ID** from the link\n",
    "3. Downloads the PDF into a local `data/` folder\n",
    "4. Saves it as `data/source.pdf`\n",
    "\n",
    "Supported link formats include:\n",
    "- `https://drive.google.com/file/d/<FILE_ID>/view?...`\n",
    "- `https://drive.google.com/open?id=<FILE_ID>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download the PDF from Google Drive\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def download_pdf_from_drive(drive_url: str, save_path: str):\n",
    "    \"\"\"\n",
    "    Download a PDF from a Google Drive sharing link and save it locally.\n",
    "    \"\"\"\n",
    "    # Try pattern: /d/<FILE_ID>/\n",
    "    match = re.search(r\"/d/([A-Za-z0-9_-]+)\", drive_url)\n",
    "    if match:\n",
    "        file_id = match.group(1)\n",
    "    else:\n",
    "        # Try pattern: ?id=<FILE_ID>\n",
    "        match = re.search(r\"id=([A-Za-z0-9_-]+)\", drive_url)\n",
    "        if match:\n",
    "            file_id = match.group(1)\n",
    "        else:\n",
    "            raise ValueError(\"‚ùå Could not extract file ID from the link.\")\n",
    "\n",
    "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "    print(f\"üì• Downloading PDF (file ID {file_id})...\")\n",
    "\n",
    "    resp = requests.get(download_url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "    print(f\"‚úÖ PDF downloaded ‚Üí {save_path}\")\n",
    "\n",
    "# Ask for the Drive link\n",
    "drive_link = input(\"üìå Paste your Google Drive PDF link here: \").strip()\n",
    "\n",
    "# Make sure the data folder exists\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Local path for the PDF\n",
    "pdf_path = os.path.join(DATA_DIR, \"source.pdf\")\n",
    "\n",
    "# Download the PDF\n",
    "download_pdf_from_drive(drive_link, pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Break the PDF into Semantic Chunks\n",
    "\n",
    "The AI cannot use one giant block of text.\n",
    "Here you:\n",
    "- Load the PDF\n",
    "- Use a **semantic splitter** to create \"smart\" chunks (not random splits)\n",
    "- Label each chunk with simple metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Break the PDF into semantic chunks\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Load the PDF as a document\n",
    "documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
    "print(f\"üìÑ Loaded {len(documents)} document(s).\")\n",
    "\n",
    "# Embedding model for semantic splitting (can reuse the same model name)\n",
    "semantic_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Create a semantic splitter\n",
    "parser = SemanticSplitterNodeParser(\n",
    "    buffer_size=3,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=semantic_embed,\n",
    ")\n",
    "\n",
    "# Generate semantic nodes (chunks)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# Add simple metadata to each chunk\n",
    "for n in nodes:\n",
    "    n.metadata[\"source\"] = pdf_path\n",
    "    n.metadata[\"chunk_type\"] = \"semantic\"\n",
    "\n",
    "print(f\"üîç Created {len(nodes)} high-quality semantic nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Build the Query Fusion Retriever\n",
    "\n",
    "Now you build the **search engine** that powers your RAG system.\n",
    "It uses **Query Fusion**:\n",
    "- Rewrites your question several ways\n",
    "- Searches multiple times\n",
    "- Fuses the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build the Query Fusion retriever\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "# Download or load the Query Fusion pack\n",
    "QueryRewritingRetrieverPack = download_llama_pack(\n",
    "    \"QueryRewritingRetrieverPack\",\n",
    "    \"./query_rewriting_pack\",\n",
    ")\n",
    "\n",
    "# Create the advanced retriever using your nodes\n",
    "query_rewriting_pack = QueryRewritingRetrieverPack(\n",
    "    nodes,                      # semantic chunks from Step 4\n",
    "    chunk_size=256,\n",
    "    vector_similarity_top_k=8,\n",
    "    fusion_similarity_top_k=8,\n",
    "    num_queries=6,              # number of query rewrites\n",
    ")\n",
    "\n",
    "print(\"üöÄ Advanced Query Fusion RAG Engine Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Ask Questions in an Interactive Loop\n",
    "\n",
    "Finally, you create a simple chat loop:\n",
    "- Type a question about the PDF\n",
    "- The system runs the RAG pipeline\n",
    "- You see a clear answer\n",
    "- Type `end` to exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Ask questions in an interactive loop\n",
    "def safe_rag_run(question, retries=3):\n",
    "    \"\"\"\n",
    "    Run the RAG pipeline with basic retry logic.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = query_rewriting_pack.run(question)\n",
    "\n",
    "            if resp is None or str(resp).strip() == \"\":\n",
    "                raise ValueError(\"Empty LLM response.\")\n",
    "\n",
    "            return resp\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "            print(f\"üîÅ Retrying ({attempt+1}/{retries})...\")\n",
    "\n",
    "    return \"‚ùå Could not generate a valid answer after retries.\"\n",
    "\n",
    "print(\"\\nRAG Interactive Mode\")\n",
    "print(\"Ask any question about your PDF.\")\n",
    "print(\"Type 'end' to exit.\\n\")\n",
    "\n",
    "# Interactive Q&A loop\n",
    "while True:\n",
    "    user_question = input(\"üü¶ Enter your question: \").strip()\n",
    "\n",
    "    if user_question.lower() == \"end\":\n",
    "        print(\"\\nüëã Session ended.\")\n",
    "        break\n",
    "\n",
    "    print(\"\\nüîç Retrieving answer...\\n\")\n",
    "\n",
    "    # Run the question through the RAG pipeline\n",
    "    response = safe_rag_run(user_question)\n",
    "\n",
    "    print(\"\\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(\"‚ùì QUESTION:\")\n",
    "    print(user_question)\n",
    "    print(\"\\nüß† ANSWER:\")\n",
    "    print(response)\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example Questions You Can Try\n",
    "\n",
    "Once everything is running, try questions like:\n",
    "\n",
    "- \"What are the main goals in this document?\"\n",
    "- \"What does this policy say about attendance?\"\n",
    "- \"Summarise the key points in chapter one.\"\n",
    "- \"List all the responsibilities of students mentioned in this document.\"\n",
    "- \"How is assessment described in this curriculum?\"\n",
    "\n",
    "For the Coffee Guide example:\n",
    "- \"What are the different coffee bean types?\"\n",
    "- \"How is cold brew made?\"\n",
    "- \"What are the health benefits of coffee?\"\n",
    "- \"Which country drinks the most coffee?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built a RAG-powered PDF Q&A assistant using Meta Llama models!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Save your work**: File ‚Üí Save a copy in Drive\n",
    "2. **Download the notebook**: File ‚Üí Download ‚Üí Download .ipynb\n",
    "3. **Push to GitLab**: Upload your completed notebook to your forked repository\n",
    "\n",
    "### Tips for GitLab:\n",
    "```bash\n",
    "# Add your notebook\n",
    "git add meta_task_pdf_qa.ipynb\n",
    "\n",
    "# Commit with a descriptive message\n",
    "git commit -m \"Complete META-powered PDF Q&A RAG system\"\n",
    "\n",
    "# Push to your fork\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Your Name  \n",
    "**Date:** December 12, 2025  \n",
    "**Framework:** LlamaIndex + Meta Llama 3.3 70B via OpenRouter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}